# 1. 熵

## 1.1 熵的概念

$$
H(X)=-\sum _i p_ilogp_i
$$

上式中， $H(X)$就是随机变量$X$的熵， $p_i$是$X$取值的离散概率。

在信息论中，熵度量了变量$X$的不确定性（**信息量**）。 XX取值的概率越分散，说明不确定性就越大，含有的信息量就越大，所以熵就越大； $X$取值的概率越集中，说明不确定性就越小，含有的信息量就越小，所以熵就越小。

极端情况下，若$X$就是一个常数，则熵为0，代表无信息量，完全确定；若$X$有$n$种取值，且每种概率都是$\frac 1 n$，则信息量最大，熵最大，为$log(n)$。
$$
H(X|Y)=\sum _j p_j H(X|y_j)
$$
上式表示在知道变量$Y$取值的情况下，变量$X$所剩余的信息量，即不确定性。

## 1.2 数据集的熵

对数据集$D$来说，我们怎么定义它的熵呢？自然地，数据集的熵就是数据集标签（监督数据）的不确定性。如果这个数据集的标签$y$都相同，自然熵就为0。假设数据集有$k$个类别，每个类比的数据集合为$D_k$，则：
$$
H(D)=-\sum _{k}\frac {|D_k|}{|D|}log(\frac {|D_k|}{|D|})
$$
数据集的熵定义了数据集里标签的分散程度，也就是不确定度，对我们来说，当然熵越小越好。那么在特征$A$对数据集进行划分以后，假设$A$有$a$个取值，把数据集划分为$\{D^1, D^2, ...,  D^a \}$那么划分后的数据集熵定义为：
$$
H(D|A)=\sum _{a} \frac {|D^a|}{|D|} H(D^a)
$$
那么，经过特征 AA进行划分之后，数据集的信息增益为：
$$
Gain(D, A)=H(D)-H(D|A)
$$
如果信息增益越大，可以认为特征$A$对数据集划分最有效。随着划分的进行，数据集的熵一定是越来越小的，也就是不确定性越来越小。



# 2. 决策树基本算法

对每一个节点进行如下操作：

- 如果此节点所有的数据标签都相同，即熵为0。则标记为叶子结点，判断为相应的标签。

- 如果已没有特征可选，或者可选的特征对降低数据集的熵无帮助。则标记为叶子结点，并且将标签判断为现有节点数据集里最多的标签。

- 否则，根据一定的策略选择最优节点$A^*$：

  - 根据$A^*$的取值将数据集划分为$a$个子数据集，分裂成相应的$a$个节点。
  - 对每个节点，若相应数据集为空，则标记为叶子结点，标签判断为母节点标签最多的那一个；否则继续分裂此节点。

  

# 3. 具体算法

## 3.1 ID3

ID3算法和基本的决策树算法一样，选择特征的策略就是选择对数据集的熵增益最大的特征。当最大增益小于某个阈值时，停止分裂，标记标签为最多的那一个标签。

### 缺点

1. 单纯根据$Gain(D,A)$来选择标签的话，有一个很严重的缺点，就是如果特征的取值越分散，算法就会越偏向这个特征。比如，两个特征的取值概率都是完全分散的，一个有两个取值，一个有三个取值，则是三个取值的增益大。
2. ID3算法没有考虑连续值的划分问题。
3. ID3算法没有考虑缺失值的处理方法。
4. ID3算法容易过拟合。

## 3.2 C4.5

针对ID3算法的四个缺点，C4.5算法提出了相应的改进：

### 惩罚取值较多的特征

针对这一点，C4.5修改了选取特征的策略，

定义特征熵：
$$
H_A(D)=-\sum _{a} \frac {|D^a|}{|D|}log(\frac {|D^a|}{|D|})
$$
这样，特征$A$的取值越分散，那么特征熵就会越大。定义信息增益比：
$$
Gain_R(D, A)=\frac {Gain(D, A)}{H_A{D}}
$$
这样，相当于对特征$A$取值分散进行了一定的惩罚。

### 处理连续值

对每个节点的连续特征进行离散化，加入属性$A$连续，在数据集$D$上有$v$个取值，那么会产生$v-1$个二类划分点，对每个划分点计算信息增益比，然后选择最大的作为属性$A$的离散划分点。

### 处理缺失值

在属性值缺失的情况下选择划分属性：只根据有这个属性值的数据计算信息增益比，然后把这个信息增益比乘上有属性值数据的比率，就可以当作这个属性整体的信息增益比了。

如何划分没有这个属性值的数据：同时划入所有子节点，但是按照各个子节点的数量来更新样本的权重。

### 处理过拟合

引入正则化系数进行剪枝。

### 缺点

1. 剪枝算法仍有优化空间。
2. C4.5会生成多叉树，在计算机中可能运行效率没有二叉树高。
3. 只能用于分类。
4. 因为有大量对熵进行的对数计算，还有对连续值划分，导致计算量非常大。

## 3.3 CART算法

### 抛弃熵模型

$$
Gini(D)=\sum_k{p_k(1-p_k)}=1 - \sum _kp_k^2
$$

基尼系数也表示了数据集标签的分散化，数据集不确定性越大，基尼系数也就越大，可以替代熵的衡量，并且降低了计算量。
$$
Gini(D, A)=\sum _a \frac {|D_a|}{|D|} Gini(D^a)
$$
同样地，如此定义的基于基尼系数的信息增益可以替代熵模型的信息增益。

### 对于离散连续特征的处理

连续特征处理基本和C4.5算法是相同的，只需要基于基尼系数选划分点及特征就好了。

对离散特征，CART选择作二类划分，找到基尼系数最小的组合。这样，这个离散特征在之后还可以作为划分特征。二分点共取$a$个，每个划分方式为$\{x = a\}, \{x{\ne a} \}$。

### 回归树

回归树和分类树的区别在于：回归树是输出连续标签，而分类树输出的是离散标签。

CART回归树基本与分类树相同，不同之处在于：

- 对于连续值的处理方式，回归树也是选所有可能的划分点，但是是使两个子集合的标签方差之和最小为选取划分点依据。
- 而对于如何预测子节点的值，是选取叶子结点的均值或者中位数。

### 剪枝算法

- 预剪枝：在决策树生成的过程中，在分裂节点之前，判断是否可以提升泛化能力，能的话继续划分，否则停止划分。一般通过验证集的准确率来衡量泛化能力。预剪枝一般计算开销较小，但是容易欠拟合。
- 后剪枝：在决策树完全生成后，从各个叶子结点从下往上收缩，如果收缩后可以提升泛化能力，则这些叶子结点全部收缩到母节点，剪掉这个分裂。后剪枝一般开销大，容易保留更多分枝。但是一般后剪枝是优于预剪枝的。

### CART的剪枝算法

- **先生成完整的二叉决策树**。
- **产生所有可能的剪枝后的CART树**。

剪枝的损失函数度量：
$$
C_{\alpha}\left(T_{t}\right)=C\left(T_{t}\right)+\alpha\left|T_{t}\right|
$$
$C\left(T_{t}\right)$代表了子树$T_t$在现行决策树下的预测误差，分类树用基尼系数，回归树用均方差之和。 $|T_t|$是子树的子节点数，二者共同组成了损失函数。

如果我们把子树$T_t$剪枝，变成一个节点$T$，则损失函数变成了：
$$
C_{\alpha}(T)=C(T)+\alpha
$$
首先，$C(T_t) < C(T)$，但是$|T_t| > 1$，所以在$\alpha$比较小的时候， $C_\alpha (T_t) > C_\alpha (T)$此时不建议剪枝，当$\alpha$不断增大时，会存在一个点使得$C_\alpha (T_t) = C_\alpha (T)$，这时，建议剪枝，此时：
$$
\alpha=\frac{C(T)-C\left(T_{t}\right)}{\left|T_{t}\right|-1}
$$
所以，对每一个内部节点，都存在这样的一个$\alpha _t$，建议剪枝。一般来说，肯定是越靠近根结点的节点，对应的$\alpha _t$越大，越难被剪枝。

- **利用交叉验证来生成泛化能力最优的决策树**。

依次选取可能的$\alpha _t$，生成相应的决策树，然后计算决策树的泛化误差，选择泛化误差最小的决策树作为最优决策树。

### 缺点

- 每次选择一个特征进行裂变，因为我们知道很多时候是由多特征一起决定的，所以单特征决策的结果可能并不是最优。
- 离散点可能会造成较大的影响。



# 4. 决策树算法的优缺点

## 4.1 优点

1. 简单直观，可解释性很强。
2. 基本不需要预处理，不需要处理缺失值，也不需要归一化。
3. 预测代价较小，为$O(log⁡2m)$，$m$为样本数。
4. 既可以处理离散值，又可以处理连续值。
5. 可以处理多分类问题。
6. 可以利用交叉验证的剪枝策略来提高决策树的泛化能力。

## 4.2 缺点

1. 决策树算法本身比较容易过拟合，可以通过限制最少样本数量和树的深度来弥补。
2. 对异常值过于敏感，决策树会因为样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习之类的方法解决。
3. 理论上的最优决策树是很难寻找的，所以都是用启发式算法来寻找的（每次寻找最优划分特征），因此容易陷入局部最优点。
4. 决策树很难学习到比较复杂的关系，比如异或。
5. 不适合处理高维数据。
6. 泛化能力太差。对于没出现过的数据基本没有办法。