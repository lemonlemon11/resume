# 1. 两种估计

## 1.1 极大似然估计

极大似然估计的目的是：根据已有的样本，**反推最有可能导致这种样本的参数值**。

极大似然估计是建立在极大似然原理的基础上的一个统计方法。假设参数是固定的，类似于通过选择模型参数，使得生成现有样本的概率最大。通常，样本都是符合独立同分布的。假设已知的样本集为：
$$
D=\vec{x}_{1}, \vec{x}_{2}, \ldots, \vec{x}_{m}
$$
那么可以得到似然函数（likelihood function）：联合概率密度函数$p(D|\vec\theta )$称为相对于$\vec x_{1},\vec x_{2},...,\vec x_{m}$的$\vec\theta$的似然函数。
$$
l(\vec\theta )=p(D|\vec\theta ) =p(\vec x_{1},\vec x_{2},...,\vec x_{m}|\vec\theta )=\prod_{i=1}^{m}p(\vec x_{i}|\vec \theta )
$$
那么，要知道$p\left(\vec{x}_{i} | \vec{\theta}\right)$则需要我们假设数据样本符合的分布类型，像LR就是假设样本符合伯努利分布得到的似然函数。这一步最重要的依据就是**数据样本独立同分布**，如果数据样本不是独立同分布的（可能出现），则会成为依赖似然函数的算法的缺点。

极大似然估计就是使似然函数（数据样本联合概率密度函数）最大的参数值：
$$
\hat{\vec\theta}=d(D)= \mathop {\arg \max}_{\vec\theta} l(\vec\theta )
$$
*那么，上式是怎么推导出来的呢？我们知道，其实我们应该求的是最大化$p(\theta|D)$，也就是在数据集$D$的情况下，参数的概率。根据概率公式，有：*
$$
p(\theta |D)=\frac {p(D|\theta)p(\theta)}{p(D)}
$$
*对于极大似然估计来说，假设$\theta$是确定的，所以$p(\theta)$是确定的，又因为$p(D)$是确定的，所以相当于最大化$p(\theta|D)$。然而在贝叶斯估计里，就不是这么一回事了*



所以，极大似然估计的前提是：

* 数据样本独立同分布，并且符合似然函数中假设的分布。
* 使得现有样本出现概率最大的参数值就是估计参数值。

## 1.2 贝叶斯估计

先验概率和后验概率的区别：

先验概率：不考虑数据样本，对参数所做的一个先验假设，$p(\theta)$。这个先验假设包含了以往的经验假设，完全与数据无关。

后验概率：考虑数据样本，参数的分布$p(\theta|D)$。

贝叶斯估计和极大似然估计的最大区别就是：没有假设参数$\theta$是固定的，而是假设$\theta$符合先验分布$p(\theta)$，那么最大化$p(\theta|D)$则可以转换为：
$$
p(\theta|D)= \frac{p(D|\theta)p(\theta)}{p(D)}= \frac {(\prod _{i=1}^{m} p(\vec x_i|\theta))p(\theta)}
{\int_{\theta} \prod _{i=1}^{m} p(\vec x_i|\theta) p(\theta) d \theta}\tag{1}
$$
其中，在化简过程中用到了以下公式：
$$
p(D|\theta)=\prod _{i=1}^{m} p(\vec x_i|\theta) \\
p(D)=\int_{\theta} p(D | \theta) p(\theta) d \theta
$$
其实，$(1)$式就是完整的贝叶斯估计结果，$p(\theta)$是我们假设的$\theta$的先验分布。只要知道数据样本符合的模型$p(\vec x_i|\theta)$就可以使上述概率（分子）最大化来估计参数了。

当然，贝叶斯估计主要是用在预测阶段的，在预测阶段$D$其实就是我们要预测的$\vec x$，公式可以简化为如下：
$$
p(\theta|\vec x)= \frac{p(\vec x|\theta)p(\theta)}{p(\vec x)}= \frac {p(\vec x|\theta)p(\theta)}
{\int_{\theta} p(\vec x|\theta) p(\theta) d \theta}
$$
贝叶斯估计学到的是参数$\theta$的分布，而不是一个值。

## 1.3 最大后验估计MAP

分母的作用是为了保证输出的是概率值（即相加为1），但其实对所有的$\theta$，分母都是相同的。所以，就有一个简化版的称为最大后验估计MAP，就是直接求解分子，并不求取概率值：
$$
\hat{\vec\theta}= \mathop {\arg \max}_{\vec\theta} p(\vec x_i|\theta)p(\theta)=\mathop {\arg \max}_{\vec\theta} (log(p(\vec x_i|\theta))+log(p(\theta)))
$$
MAP学到的是参数$\theta$的一个最优值。



## 1.3 异同

相同点：

* 二者都是参数化估计，即假设了数据样本符合的分布类型，直接估计参数。
* 假设样本之间相互独立。



不同点：

* 极大似然估计假设参数是一个确定的数字，直接来估计使数据样本出现概率最大的参数；贝叶斯估计假设参数是一个随机变量，并且符合一个先验分布$p(\theta)$，通过极大似然概率和先验概率的乘积来求解后验概率。极大似然概率则没有考虑模型本身的概率，或者就是假设模型出现的概率都相等。
* 贝叶斯估计复杂度大于极大似然估计。
* 极大似然估计仅仅在现有样本下对数据总体分布进行估计在数据量不大的情况下可能效果不好；贝叶斯估计则考虑了参数的先验分布。
* 贝叶斯估计学到的是参数$\theta$的分布，极大似然估计和MAP学到的都是一个具体解。



# 2. 贝叶斯分类器

贝叶斯决策论在假设相关概率已知的情况下，利用误判损失来选择最优的类别分类。

假设$Y={c_1, c_2, …, c_k}$共有$k$个类别，那么贝叶斯分类器就是判断：对于样本$\vec x$，属于哪一类？

根据贝叶斯估计，我们可以得到公式：
$$
P(c_i|\boldsymbol{x})=\frac{P(\vec{x}|c_i)P(c_i)}{P(\vec{x})}
$$
可以对应出：$P(c_i)$其实就是参数的先验概率分布，而分母$P(\vec x)$其实对于固定的$\vec x$是确定的，所以，相当于求最大的：
$$
\mathop {\arg \max}_{c_i}  P(\vec x|c_i)P(c_i)
$$
所以，贝叶斯分类器的步骤可以概括如下：

* 算出样本$\vec x$属于每一类的概率$P(c_i|\vec x)$。
* 比较所有的$P(c_i|\vec x)$，选择最大的值作为$\vec x$的分类结果$c_i$。



# 3. 朴素贝叶斯分类器

假设$\vec{x}=\{ x_1,x_2,...,x_d\}$，对于贝叶斯分类器来说：

$P(c_i)$：可以用每一个类别占所有样本的比例来计算。

$P(\vec x|c_i)$：则很难计算，因为是一个$d$变量的分布。

朴素贝叶斯则假设**特征各个维度相互独立**，则可以得到：
$$
P(\vec x|c_i)=\prod_{j=1}^{d}P(x_j|c_i)
$$
可以得到$P(\vec x|c_i)$。而对于$P(x_j|c_i)$，对离散属性来说，可以根据频率来估计，对于连续属性来说，可以假设这个属性符合正态分布，利用概率密度来估计。
$$
P\left(x_{j} | c_{i}\right)=\frac{P\left(x_{j}, c_{i}\right)}{P\left(c_{i}\right)} \approx \frac{\#\left(x_{j}, c_{i}\right)}{\#\left(c_{i}\right)}
$$
由于未观测到属性概率会为0，所以一般会加上一个修正：使其最小出现次数为1。



如何估计$P(x_j|c_i)$，除了对离散属性用频率估计以外，根据估计方法的不同，可以大致分为以下几个模型。不同的模型对$P(x_j|c_i)$的假设分布不同。

## 3.1 高斯朴素贝叶斯模型

高斯朴素贝叶斯模型假设$P(x_j|c_i)$的分布为高斯分布：
$$
P\left(x_{j} | c_i\right)=\frac{1}{\sqrt{2 \pi \sigma_{c_i}^{2}}} \exp \left(-\frac{\left(x_{j}-\mu_{c_i}\right)^{2}}{2 \sigma_{c_i}^{2}}\right)
$$
高斯模型要求输入特征为连续变量。并且假设每一类$c_{i}$的数据都符合高斯分布，均值和方差可以用样本来估计。

## 3.2 伯努利朴素贝叶斯模型

伯努利朴素贝叶斯模型假设$P(x_j|c_i)$符合伯努利分布：
$$
P\left(x_{j} =1| c_{i}\right)= p_{ij}
$$
$p_{ij}$同样可以使用频率来估计。以文本分类为例：输入文本可以表示为单词的集合，也就是一个维度为$V$的向量，若文本含有某个单词，则对应位置为1，否则为0。就可以使用贝叶斯分类器对文本进行分类。



## 3.3 多项式朴素贝叶斯模型

多项式朴素贝叶斯模型假设$P(\vec {x}|c_i)$符合多项式分布（指定文本分类）：
$$
\operatorname{P}\left(\vec x | c_i\right)=\left(\sum_{j} \# x_{ji}\right) ! \prod_{j} \frac{\operatorname{P}\left(x_{ji} | c_i\right)^{\# x_{ji}}}{\# x_{ji} !}
$$
其中$P(x_j|c_i)$表示单词$x_j$在$c_i$中出现的概率，可以使用频率$\frac{\#\left(x_{j}, c_{i}\right)}{\#\left(c_{i}\right)}$来估计。这个模型的来源是：假设文本是单词的集合，则这些单词生成文本是类$c_i$的概率（考虑顺序）为：
$$
\prod _j P(x_{ji} | c_i)^{\# x_{ji}}
$$
如何考虑顺序，这些单词可以组合成为以下这么多种不同的文本：
$$
\frac {\left(\sum_{j} \# x_{ji}\right) !} {\prod_{j} \# x_{ji} !}
$$
二者相乘就是这些单词组合成为的文本 $\vec {x}$ 属于类 $c_{i}$ 的概率。

这样就用上了单词频率的信息，比伯努利模型效果要好一些。

而预测类的时候，单词组合的不同数量适合类没有关系的，因此预测的时候直接使用下式就可以了。

$$
\hat{c}=\arg \max _{c} (\prod _j P(x_{j} | c)^{\# x_{j}})
$$


















