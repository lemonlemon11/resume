# 1. 什么是逻辑回归

逻辑回归 (Logistic Regression, LR)是一种二分类方法，其实是类似于一个两层的神经网络，采取多分类交叉熵损失函数。

# 2. 原理推导

假设存在一组数据 $(x_1, y_1), (x_2, y_2), ..., (x_m, y_m)$，其中 $x_i = [x_{i1}, x_{i2}, ..., x_{in}], y_i = \{0, 1\}$。那么逻辑回归要做的工作就是构建一个模型，从自变量 $X$来预测 $Y$：
$$
\tilde{y}_{i}=Sigmoid(w_{0}+w_{1} x_{i 1}+\dots+w_{n} x_{i n})=Sigmoid(w^{T} x_{i})= \frac{e^{w^{T} x_{i}}}{1+e^{w^{T} x_i}}
$$
$\tilde{y}_{i}$就是预测的目标概率，我们可以认为：
$$
P(y_i = 1|x_i)= \frac{e^{w^{T} x_{i}}}{1+e^{w^{T} x_i}} ,\qquad P(y_i=0|x_i)=\frac{1}{1+e^{w^{T} x_i}}
$$
逻辑回归采用的更新策略是最大化对数似然函数，其实是和最小化对数损失函数完全一样的。

似然函数为：
$$
L(w)=\prod_{i=1}^{m}{(\frac{e^{w^{T} x_{i}}}{1+e^{w^{T} x_i}})}^{y_i}{(\frac{1}{1+e^{w^{T} x_i}})}^{(1-y_i)}
$$
对应的对数似然函数为：
$$
ln(L(w))=\sum_{i=1}^{m}y_i ln(\frac{e^{w^{T} x_{i}}}{1+e^{w^{T} x_i}})) + (1-y_i)(\frac{1}{1+e^{w^{T} x_i}})
$$
而二分类交叉熵损失函数则为：
$$
J(w)=-\frac {1}{N}ln(L(w))
$$
对参数 $w$进行梯度更新：
$$
w=w - x_i(y_i - \frac{e^{w^{T} x_{i}}}{1+e^{w^{T} x_i}})
$$
可以发现最小化损失函数和最大化对数似然函数的效果是完全一样的。训练的时候用梯度下降法就可以了。

#3. 一些问题

- **为什么要使用最大似然函数？**
  - 最大似然函数目的是利用数据的概率分布，来反推最有可能导致这种概率分布的参数值。即我们想要最大化数据同时出现的总概率。为了使计算变得简单，一般会使用对数似然函数。
  - 梯度更新比较快，可以看出梯度更新和 $Sigmoid$的梯度无关。

- **为什么不使用均方误差**？
  - 均方误差是非凸函数。
  - 均方误差的梯度更新依赖于$Sigmoid$的梯度，所以更新会比较慢。
  - 均方误差和最大似然估计原理不同，后者依赖概率最大化，前者是误差最小化。

- **逻辑回归的优点**
  - 模型清晰简单，推导过程明确，模型参数具有可解释性。
  - 输出具有概率意义。
  - 计算量小，可以适应大数据。
  - 对于新来的数据可以再次训练，不用重新开始训练。
  - 不需要各个特征满足条件独立假设。

- **逻辑回归的缺点**
  - 本质还是一个线性分类器，要求数据线性可分。
  - 算法复杂度依赖于特征的数量，特征数量过多时性能不好。
  - 容易欠拟合，精度不高。
  - 假设数据符合伯努利分布。
  - 很难处理数据不平衡的情况。

- **高度相关特征会影响结果吗？**
  - 不会，就算一个特征被重复多次，也不会影响结果，只是会在参数上显示出来。
  - 一般还是会去掉高度相关参数，为了训练速度和模型可解释性。